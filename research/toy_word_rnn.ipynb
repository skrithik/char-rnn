{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b51e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like deep learning',\n",
       " 'i like machine learning',\n",
       " 'i love deep learning',\n",
       " 'i love machine learning']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"i like deep learning\",\n",
    "    \"i like machine learning\",\n",
    "    \"i love deep learning\",\n",
    "    \"i love machine learning\"\n",
    "]\n",
    "\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f46b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"i like deep learning\",\n",
    "    \"i like machine learning\",\n",
    "    \"i like neural networks\",\n",
    "    \"i like data science\",\n",
    "    \"i like artificial intelligence\",\n",
    "\n",
    "    \"i love deep learning\",\n",
    "    \"i love machine learning\",\n",
    "    \"i love neural networks\",\n",
    "    \"i love data science\",\n",
    "    \"i love artificial intelligence\",\n",
    "\n",
    "    \"we like deep learning\",\n",
    "    \"we like machine learning\",\n",
    "    \"we like neural networks\",\n",
    "    \"we like data science\",\n",
    "    \"we like artificial intelligence\",\n",
    "\n",
    "    \"we love deep learning\",\n",
    "    \"we love machine learning\",\n",
    "    \"we love neural networks\",\n",
    "    \"we love data science\",\n",
    "    \"we love artificial intelligence\",\n",
    "\n",
    "    \"they like deep learning\",\n",
    "    \"they like machine learning\",\n",
    "    \"they like neural networks\",\n",
    "    \"they like data science\",\n",
    "    \"they like artificial intelligence\",\n",
    "\n",
    "    \"they love deep learning\",\n",
    "    \"they love machine learning\",\n",
    "    \"they love neural networks\",\n",
    "    \"they love data science\",\n",
    "    \"they love artificial intelligence\",\n",
    "\n",
    "    \"students like deep learning\",\n",
    "    \"students like machine learning\",\n",
    "    \"students like neural networks\",\n",
    "    \"students like data science\",\n",
    "    \"students like artificial intelligence\",\n",
    "\n",
    "    \"researchers love deep learning\",\n",
    "    \"researchers love machine learning\",\n",
    "    \"researchers love neural networks\",\n",
    "    \"researchers love data science\",\n",
    "    \"researchers love artificial intelligence\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72833346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'like', 'deep', 'learning'],\n",
       " ['i', 'like', 'machine', 'learning'],\n",
       " ['i', 'like', 'neural', 'networks'],\n",
       " ['i', 'like', 'data', 'science'],\n",
       " ['i', 'like', 'artificial', 'intelligence'],\n",
       " ['i', 'love', 'deep', 'learning'],\n",
       " ['i', 'love', 'machine', 'learning'],\n",
       " ['i', 'love', 'neural', 'networks'],\n",
       " ['i', 'love', 'data', 'science'],\n",
       " ['i', 'love', 'artificial', 'intelligence'],\n",
       " ['we', 'like', 'deep', 'learning'],\n",
       " ['we', 'like', 'machine', 'learning'],\n",
       " ['we', 'like', 'neural', 'networks'],\n",
       " ['we', 'like', 'data', 'science'],\n",
       " ['we', 'like', 'artificial', 'intelligence'],\n",
       " ['we', 'love', 'deep', 'learning'],\n",
       " ['we', 'love', 'machine', 'learning'],\n",
       " ['we', 'love', 'neural', 'networks'],\n",
       " ['we', 'love', 'data', 'science'],\n",
       " ['we', 'love', 'artificial', 'intelligence'],\n",
       " ['they', 'like', 'deep', 'learning'],\n",
       " ['they', 'like', 'machine', 'learning'],\n",
       " ['they', 'like', 'neural', 'networks'],\n",
       " ['they', 'like', 'data', 'science'],\n",
       " ['they', 'like', 'artificial', 'intelligence'],\n",
       " ['they', 'love', 'deep', 'learning'],\n",
       " ['they', 'love', 'machine', 'learning'],\n",
       " ['they', 'love', 'neural', 'networks'],\n",
       " ['they', 'love', 'data', 'science'],\n",
       " ['they', 'love', 'artificial', 'intelligence'],\n",
       " ['students', 'like', 'deep', 'learning'],\n",
       " ['students', 'like', 'machine', 'learning'],\n",
       " ['students', 'like', 'neural', 'networks'],\n",
       " ['students', 'like', 'data', 'science'],\n",
       " ['students', 'like', 'artificial', 'intelligence'],\n",
       " ['researchers', 'love', 'deep', 'learning'],\n",
       " ['researchers', 'love', 'machine', 'learning'],\n",
       " ['researchers', 'love', 'neural', 'networks'],\n",
       " ['researchers', 'love', 'data', 'science'],\n",
       " ['researchers', 'love', 'artificial', 'intelligence']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = [s.split() for s in sentences]\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0862962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['artificial',\n",
       "  'data',\n",
       "  'deep',\n",
       "  'i',\n",
       "  'intelligence',\n",
       "  'learning',\n",
       "  'like',\n",
       "  'love',\n",
       "  'machine',\n",
       "  'networks',\n",
       "  'neural',\n",
       "  'researchers',\n",
       "  'science',\n",
       "  'students',\n",
       "  'they',\n",
       "  'we'],\n",
       " 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(word for sent in tokenized for word in sent))\n",
    "vocab, len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d208c20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artificial': 0,\n",
       " 'data': 1,\n",
       " 'deep': 2,\n",
       " 'i': 3,\n",
       " 'intelligence': 4,\n",
       " 'learning': 5,\n",
       " 'like': 6,\n",
       " 'love': 7,\n",
       " 'machine': 8,\n",
       " 'networks': 9,\n",
       " 'neural': 10,\n",
       " 'researchers': 11,\n",
       " 'science': 12,\n",
       " 'students': 13,\n",
       " 'they': 14,\n",
       " 'we': 15}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix = {w: i for i, w in enumerate(vocab)}\n",
    "ix_to_word = {i: w for w, i in word_to_ix.items()}\n",
    "\n",
    "word_to_ix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d9165c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 6, 2, 5],\n",
       " [3, 6, 8, 5],\n",
       " [3, 6, 10, 9],\n",
       " [3, 6, 1, 12],\n",
       " [3, 6, 0, 4],\n",
       " [3, 7, 2, 5],\n",
       " [3, 7, 8, 5],\n",
       " [3, 7, 10, 9],\n",
       " [3, 7, 1, 12],\n",
       " [3, 7, 0, 4],\n",
       " [15, 6, 2, 5],\n",
       " [15, 6, 8, 5],\n",
       " [15, 6, 10, 9],\n",
       " [15, 6, 1, 12],\n",
       " [15, 6, 0, 4],\n",
       " [15, 7, 2, 5],\n",
       " [15, 7, 8, 5],\n",
       " [15, 7, 10, 9],\n",
       " [15, 7, 1, 12],\n",
       " [15, 7, 0, 4],\n",
       " [14, 6, 2, 5],\n",
       " [14, 6, 8, 5],\n",
       " [14, 6, 10, 9],\n",
       " [14, 6, 1, 12],\n",
       " [14, 6, 0, 4],\n",
       " [14, 7, 2, 5],\n",
       " [14, 7, 8, 5],\n",
       " [14, 7, 10, 9],\n",
       " [14, 7, 1, 12],\n",
       " [14, 7, 0, 4],\n",
       " [13, 6, 2, 5],\n",
       " [13, 6, 8, 5],\n",
       " [13, 6, 10, 9],\n",
       " [13, 6, 1, 12],\n",
       " [13, 6, 0, 4],\n",
       " [11, 7, 2, 5],\n",
       " [11, 7, 8, 5],\n",
       " [11, 7, 10, 9],\n",
       " [11, 7, 1, 12],\n",
       " [11, 7, 0, 4]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = [[word_to_ix[w] for w in sent] for sent in tokenized]\n",
    "encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39998f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "V = len(vocab)\n",
    "\n",
    "def one_hot(idx, V):\n",
    "    v = np.zeros(V)\n",
    "    v[idx] = 1\n",
    "    return v\n",
    "\n",
    "one_hot(encoded[0][0], V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28dba63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "V = len(vocab)\n",
    "\n",
    "def one_hot(idx, V):\n",
    "    v = torch.zeros(V)\n",
    "    v[idx] = 1.0\n",
    "    return v\n",
    "\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for sent in encoded:\n",
    "    X_sent = []\n",
    "    Y_sent = []\n",
    "    \n",
    "    for t in range(len(sent) - 1):\n",
    "        X_sent.append(one_hot(sent[t], V))   # torch tensor (V,)\n",
    "        Y_sent.append(sent[t + 1])           # python int\n",
    "    \n",
    "    X_train.append(torch.stack(X_sent))              # (T, V)\n",
    "    Y_train.append(torch.tensor(Y_sent, dtype=torch.long))  # (T,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebecec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454c3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad4f1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.V = vocab_size\n",
    "        self.H = hidden_size\n",
    "        \n",
    "        self.Wxh = nn.Linear(vocab_size, hidden_size, bias=True)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Why = nn.Linear(hidden_size, vocab_size, bias=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (T, V) one-hot tensor\n",
    "        \"\"\"\n",
    "        T = X.shape[0]\n",
    "        h = torch.zeros(self.H)   # h0\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            x_t = X[t]\n",
    "            h = torch.tanh(self.Wxh(x_t) + self.Whh(h))\n",
    "            y_t = self.Why(h)\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        return torch.stack(outputs)  # (T, V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b73b080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14808\\1265691402.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X0 = torch.tensor(X_train[0], dtype=torch.float32)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14808\\1265691402.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y0 = torch.tensor(Y_train[0], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = len(vocab)\n",
    "H = 16\n",
    "\n",
    "model = SimpleRNN(V, H)\n",
    "\n",
    "X0 = torch.tensor(X_train[0], dtype=torch.float32)\n",
    "Y0 = torch.tensor(Y_train[0], dtype=torch.long)\n",
    "\n",
    "logits = model(X0)\n",
    "\n",
    "logits.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "298cafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.V = vocab_size\n",
    "        self.H = hidden_size\n",
    "        \n",
    "        self.Wxh = nn.Linear(vocab_size, hidden_size)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Why = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (T, V)\n",
    "        returns logits: (T, V)\n",
    "        \"\"\"\n",
    "        T = X.shape[0]\n",
    "        h = torch.zeros(self.H)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            x_t = X[t]                       # (V,)\n",
    "            h = torch.tanh(\n",
    "                self.Wxh(x_t) + self.Whh(h)  # (H,)\n",
    "            )\n",
    "            y_t = self.Why(h)                # (V,)\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        return torch.stack(outputs)          # (T, V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3e5eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 8  # hidden size\n",
    "\n",
    "model = SimpleRNN(V, H)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b94d3da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 16]), 2.574345588684082)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0 = X_train[0]\n",
    "Y0 = Y_train[0]\n",
    "\n",
    "logits = model(X0)\n",
    "loss = F.cross_entropy(logits, Y0)\n",
    "\n",
    "logits.shape, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7979de3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Why.out_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f21700d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a207df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 96.9898\n",
      "Epoch 50, Loss: 29.1653\n",
      "Epoch 100, Loss: 28.9579\n",
      "Epoch 150, Loss: 28.8823\n",
      "Epoch 200, Loss: 28.8587\n",
      "Epoch 250, Loss: 28.8386\n",
      "Epoch 300, Loss: 28.8075\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for X, Y in zip(X_train, Y_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X)              # forward\n",
    "        loss = F.cross_entropy(logits, Y)\n",
    "        \n",
    "        loss.backward()                # BPTT\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "568cc94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, start_words, steps=5):\n",
    "    model.eval()\n",
    "    \n",
    "    words = start_words.split()\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        X = torch.stack([\n",
    "            one_hot(word_to_ix[w], V) for w in words\n",
    "        ])\n",
    "        \n",
    "        logits = model(X)\n",
    "        probs = F.softmax(logits[-1], dim=0)\n",
    "        next_idx = torch.argmax(probs).item()\n",
    "        words.append(ix_to_word[next_idx])\n",
    "    \n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7ef0cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like neural networks like deep learning'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"i like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96e95207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'they love neural networks like deep learning'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"they love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02ac060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'students like data science love neural networks'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, \"students like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b17d210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love neural networks like deep learning'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model,\"i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbfcc4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like like deep learning love neural'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model,\"like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "528cb5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'researchers love data science love neural networks'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model,\"researchers love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819e5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
